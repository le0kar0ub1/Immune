"""Simple MLP for malware detection with training and evaluation."""

import logging
from pathlib import Path
from typing import Dict, List

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

logger = logging.getLogger(__name__)


class MalwareDetector(nn.Module):
    """Simple fully connected MLP for malware detection."""

    def __init__(self, layers: List[int] = [356, 128, 128, 64, 1], dropout_rate: float = 0.5):
        super().__init__()

        self.input_size = layers[0]
        self.hidden_size = layers[1]
        self.dropout_rate = dropout_rate

        self.network = nn.ModuleList()

        for i in range(len(layers) - 1):
            self.network.append(nn.Linear(layers[i], layers[i + 1]))
            self.network.append(nn.ReLU())
            self.network.append(nn.Dropout(dropout_rate))

        self.network.append(nn.Linear(self.hidden_size, 1))
        self.network.append(nn.Sigmoid())

        logger.info(f"Initialized MalwareDetector: {layers}")
        logger.info(f"Network: {self.network}")
        logger.info(f"Total parameters: {sum(p.numel() for p in self.parameters()):,}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

    def predict(self, x: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:
        with torch.no_grad():
            probabilities = self.forward(x)
            return (probabilities > threshold).float()

    def get_feature_importance(self, x: torch.Tensor) -> torch.Tensor:
        x.requires_grad_(True)
        output = self.forward(x)
        output.backward(torch.ones_like(output))
        importance = torch.abs(x.grad)
        return importance

    def save_model(self, filepath: Path) -> None:
        torch.save(
            {
                "model_state_dict": self.state_dict(),
                "input_size": self.input_size,
                "hidden_size": self.hidden_size,
                "dropout_rate": self.dropout_rate,
            },
            filepath,
        )
        logger.info(f"Model saved to {filepath}")

    @classmethod
    def load_model(cls, filepath: Path) -> "MalwareDetector":
        checkpoint = torch.load(filepath, map_location="cpu")
        model = cls(
            input_size=checkpoint["input_size"],
            hidden_size=checkpoint["hidden_size"],
            dropout_rate=checkpoint["dropout_rate"],
        )
        model.load_state_dict(checkpoint["model_state_dict"])
        logger.info(f"Model loaded from {filepath}")
        return model


class MalwareDataset:
    """Dataset class for malware detection."""

    def __init__(self, features: np.ndarray, labels: np.ndarray):
        self.features = torch.FloatTensor(features)
        self.labels = torch.FloatTensor(labels)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


def train_model(
    model: MalwareDetector,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int = 100,
    learning_rate: float = 0.001,
    device: str = "cpu",
    early_stopping_patience: int = 15,
) -> Dict[str, List[float]]:
    """Train the malware detection model."""
    model = model.to(device)
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=10, verbose=True
    )

    train_losses = []
    val_losses = []
    best_val_loss = float("inf")
    patience_counter = 0

    logger.info(f"Starting training for {epochs} epochs on {device}")

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels.unsqueeze(1))
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predictions = (outputs > 0.5).float()
            train_correct += (predictions.squeeze() == labels).sum().item()
            train_total += labels.size(0)

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for features, labels in val_loader:
                features, labels = features.to(device), labels.to(device)
                outputs = model(features)
                loss = criterion(outputs, labels.unsqueeze(1))
                val_loss += loss.item()

                predictions = (outputs > 0.5).float()
                val_correct += (predictions.squeeze() == labels).sum().item()
                val_total += labels.size(0)

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_accuracy = 100 * train_correct / train_total
        val_accuracy = 100 * val_correct / val_total

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)

        # Learning rate scheduling
        scheduler.step(avg_val_loss)

        # Early stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1

        if epoch % 10 == 0:
            logger.info(
                f"Epoch {epoch:3d}: "
                f"Train Loss: {avg_train_loss:.4f} (Acc: {train_accuracy:.2f}%), "
                f"Val Loss: {avg_val_loss:.4f} (Acc: {val_accuracy:.2f}%)"
            )

        # Early stopping check
        if patience_counter >= early_stopping_patience:
            logger.info(f"Early stopping triggered at epoch {epoch}")
            break

    logger.info("Training completed")
    return {"train_losses": train_losses, "val_losses": val_losses}


def evaluate_model(
    model: MalwareDetector, test_loader: DataLoader, device: str = "cpu"
) -> Dict[str, float]:
    """Evaluate the trained model."""
    model.eval()
    all_predictions = []
    all_labels = []
    test_loss = 0.0
    criterion = nn.BCELoss()

    with torch.no_grad():
        for features, labels in test_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = model(features)
            loss = criterion(outputs, labels.unsqueeze(1))
            test_loss += loss.item()

            predictions = (outputs > 0.5).float()
            all_predictions.extend(predictions.squeeze().cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate metrics
    from sklearn.metrics import (
        accuracy_score,
        f1_score,
        precision_score,
        recall_score,
        roc_auc_score,
    )

    accuracy = accuracy_score(all_labels, all_predictions)
    precision = precision_score(all_labels, all_predictions)
    recall = recall_score(all_labels, all_predictions)
    f1 = f1_score(all_labels, all_predictions)

    # For ROC AUC, we need the raw probabilities
    model.eval()
    all_probabilities = []
    all_labels_for_auc = []

    with torch.no_grad():
        for features, labels in test_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = model(features)
            all_probabilities.extend(outputs.squeeze().cpu().numpy())
            all_labels_for_auc.extend(labels.cpu().numpy())

    try:
        auc = roc_auc_score(all_labels_for_auc, all_probabilities)
    except ValueError:
        auc = 0.0

    metrics = {
        "test_loss": test_loss / len(test_loader),
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1,
        "roc_auc": auc,
    }

    logger.info("Evaluation completed")
    for metric, value in metrics.items():
        logger.info(f"{metric}: {value:.4f}")

    return metrics
