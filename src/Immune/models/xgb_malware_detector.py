import torch
import numpy as np
from torch.utils.data import DataLoader
import xgboost as xgb
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Helper: Convert a PyTorch Dataset into numpy arrays
def dataset_to_numpy(loader):
    # loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)
    X_list, y_list = [], []
    for X, y in loader:
        X_list.append(X.numpy())
        y_list.append(y.numpy())
    X = np.concatenate(X_list, axis=0)
    y = np.concatenate(y_list, axis=0)
    return X, y

def plot_datasets(train_loader, val_loader, test_loader):
    X_train, y_train = dataset_to_numpy(train_loader)
    X_val, y_val     = dataset_to_numpy(val_loader)
    X_test, y_test   = dataset_to_numpy(test_loader)
    
    print(X_train.shape, y_train.shape)
    print(X_val.shape, y_val.shape)
    print(X_test.shape, y_test.shape)
    
    print(X_train[:10, :])
    
    # Create subplots side by side
    # fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Plot training data
    # sns.boxplot(data=X_train[:10, :].T, showfliers=False, ax=axes[0])
    # axes[0].set_title('Training Data')
    # axes[0].set_xlabel('Features')
    # axes[0].set_ylabel('Values')
    
    # # Plot validation data
    # sns.boxplot(data=X_val[:10, :].T, showfliers=False, ax=axes[1])
    # axes[1].set_title('Validation Data')
    # axes[1].set_xlabel('Features')
    # axes[1].set_ylabel('Values')
    
    # # Plot test data
    # sns.boxplot(data=X_test[:10, :].T, showfliers=False, ax=axes[2])
    # axes[2].set_title('Test Data')
    # axes[2].set_xlabel('Features')
    # axes[2].set_ylabel('Values')

    sns.histplot(data=y_val)
    # plt.tight_layout()
    plt.show()
    exit(0)
    

def train_xgb_model(train_loader, val_loader, test_loader):
    # plot_datasets(train_loader, val_loader, test_loader)
    # Convert datasets
    X_train, y_train = dataset_to_numpy(train_loader)
    X_val, y_val     = dataset_to_numpy(val_loader)
    X_test, y_test   = dataset_to_numpy(test_loader)

    # XGBoost uses DMatrix for efficiency
    # random_arr = np.random.rand(*X_train.shape)
    # dtrain = xgb.DMatrix(random_arr, label=y_train)
    # dtrain = xgb.DMatrix(X_train[:, :1], label=y_train)
    # dval   = xgb.DMatrix(X_val[:, :1], label=y_val)
    # dtest  = xgb.DMatrix(X_test[:, :1], label=y_test)
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval   = xgb.DMatrix(X_val, label=y_val)
    dtest  = xgb.DMatrix(X_test, label=y_test)

    # Training parameters (binary classification example)
    params = {
        "objective": "binary:logistic",  # change to "multi:softmax" if multi-class
        "eval_metric": ["logloss", "error", "auc", "aucpr"],        # could also use "error" (1-acc)
        "eta": 0.1,
        "max_depth": 6,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "seed": 42,
    }

    evals_result = {}

    # Training with early stopping
    evals = [(dtrain, "train"), (dval, "val")]
    model = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=2000,
        evals=evals,
        early_stopping_rounds=50,
        verbose_eval=20,
        evals_result=evals_result,
    )

    # Predictions on test
    y_pred_prob = model.predict(dtest)
    y_pred = (y_pred_prob > 0.5).astype(int)

    # Accuracy
    acc = accuracy_score(y_test, y_pred)
    print(f"Test Accuracy: {acc:.4f}")

    return evals_result
